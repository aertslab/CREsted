# !!! DON'T EDIT THIS FILE !!! 
# Default configuration file for the project.
# Instead, create a user.yml file in /configs and
# copy the contents of this file into the user.yml file.
# Add/overwrite the values in the user.yml file.

# general
project_name: 'mouse'  # used for naming output folders
num_classes: 19
seq_len: 2114
target_len: 1000
output_dir : ''  # output directory for model checkpoints and logs, leave empty for default ('DeepPeak/checkpoints')

# Train/val/test
val: ['chr8', 'chr10']  # leave list empty for auto val chrom selection (0.1 of data)
test: ['chr9', 'chr18']  # leave list empty for auto test chrom selection (0.1 of data)

# Preprocessing
gini_normalization: False  # gini normalization of target peaks
specificity_filtering: False  # filter regions based on region specificity

# General Training configs
task: ''  # !!REQUIRED!! name of the task. 'deeppeak' or 'deeptopic'
wandb: True
wandb_project: '' # Name of Weights & Biases project. Leave empty for automatic naming based on project_name.
run_name: '' # Name of run used for both W&B and output folder for checkpoints, leave empty for start datetime as title
epochs: 100
batch_size: 256
TL_batch_size : 64  # batch_size for transfer learning
learning_rate: 0.001
TL_learning_rate: 0.0001  # learning rate for transfer learning
patience: 10  # number of epochs to wait before early stopping
pretrained_model_path: ''  # continue from .keras checkpoint. Leave empty if starting from scratch
shuffle: True
mixed_precision: True  # use mixed precision training (recent GPUs only)

# Tasks specific configs
DeepPeak:
  loss: 'mse_cosine'  # mse_cosine, mse_cosine_log, mse_cosine_nk
  cosine_weight: 'static' # Leave weight of cosine function fixed ('static') or equal to the magnitude of the MSE ('dynamic')
  model_architecture: 'chrombpnet'  # simple_convnet, basenji, chrombpnet
  target: 'mean'  # max, mean, count, or logcount

DeepTopic:
  loss : 'crossentropy'
  model_architecture: 'deeptopiccnn'  # deeptopiccnn, deeptopiclstm

# augmentations (online)
rev_complement: False # reverse complement all training regions during every epoch
augment_complement: True  # inverse complement base pairs during training (50 % chance)
augment_shift_n_bp: 0  # stochastic shift up to n base pairs without recalculating targets

# Debugging
fraction_of_data: 1.0  # fraction of data to use for training. Only lower for debugging.
profile: False  # profile training with TensorBoard profiler
seed: 0  # set seed for reproducibility. Set to 0 for random seed

# Model architectures
simple_convnet:
  num_conv_blocks: 3
  num_dense_blocks: 2
  residual: 0
  first_activation: 'exponential'
  activation: 'swish'
  output_activation: 'softplus'
  normalization: 'batch'
  first_filters: 192
  filters: 256
  first_kernel_size: 13
  kernel_size: 7
  first_pool_size: 8
  pool_size: 2
  conv_dropout: 0.1
  dense_dropout: 0.3
  flatten: True
  dense_size: 256
  bottleneck: 8

chrombpnet:
  ## First convblock
  first_conv_filters: 512
  first_conv_filter_size: 5
  first_conv_pool_size: 0
  first_conv_activation: 'gelu'
  first_conv_l2: 0.00001
  first_conv_dropout: 0.1
  first_conv_res: False

  ## Dilated Conv blocks after first conv layer
  num_filters: 512
  n_dil_layers: 8
  filter_size: 3
  activation: 'relu'  # 'activation_name' or 'none'
  l2: 0.00001
  dropout: 0.1  # set to 0.0 for no dropout
  batch_norm: True
  dense_bias: True

basenji:
  first_activation: 'gelu'
  activation: 'gelu'
  output_activation: 'softplus'
  first_filters: 256
  filters: 2048
  first_kernel_size: 15
  kernel_size: 3

deeptopiccnn:
  filters: 1024
  first_kernel_size: 17
  pool_size: 4
  dense_out: 1024
  first_activation: 'gelu'
  activation: 'relu'
  normalization: 'batch'
  conv_do: 0.15
  dense_do: 0.5
  pre_dense_do: 0.5
  first_kernel_l2: 0.0001
  kernel_l2: 0.00001